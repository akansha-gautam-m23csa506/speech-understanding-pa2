{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Understanding\n",
    "## Assignment 2\n",
    "#### Question 1.3\n",
    "### Akansha Gautam (M23CSA506)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import subprocess\n",
    "from transformers import Wav2Vec2Processor, HubertForCTC\n",
    "from sklearn.metrics import roc_curve\n",
    "from transformers import AutoModel, AutoProcessor, AutoFeatureExtractor\n",
    "from speechbrain.inference.separation import SepformerSeparation as separator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile as wav\n",
    "from mir_eval.separation import bss_eval_sources\n",
    "from pesq import pesq\n",
    "import torch.nn.functional as F\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2Processor, WavLMModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import UniSpeechSatModel, UniSpeechSatConfig, Wav2Vec2FeatureExtractor\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import brentq\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pytube import YouTube\n",
    "import ffmpeg\n",
    "import os\n",
    "import loralib as lora\n",
    "from loralib import Linear as LoRALinear\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load VoxCeleb2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vox2 shape: (36237, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>abs_filepath</th>\n",
       "      <th>rel_filepath</th>\n",
       "      <th>identity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00043.m4a</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id00017/OLguY5ofUrY/00043.m4a</td>\n",
       "      <td>../id00017/OLguY5ofUrY/00043.m4a</td>\n",
       "      <td>id00017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00179.m4a</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id00017/lvX8diWgriA/00179.m4a</td>\n",
       "      <td>../id00017/lvX8diWgriA/00179.m4a</td>\n",
       "      <td>id00017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name  \\\n",
       "0  00043.m4a   \n",
       "1  00179.m4a   \n",
       "\n",
       "                                                                                                       abs_filepath  \\\n",
       "0  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id00017/OLguY5ofUrY/00043.m4a   \n",
       "1  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id00017/lvX8diWgriA/00179.m4a   \n",
       "\n",
       "                       rel_filepath identity  \n",
       "0  ../id00017/OLguY5ofUrY/00043.m4a  id00017  \n",
       "1  ../id00017/lvX8diWgriA/00179.m4a  id00017  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the vox celeb2 audio files path\n",
    "vox2_path = '/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac'\n",
    "data = []\n",
    "\n",
    "for root, dirs, files in os.walk(vox2_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".m4a\"):\n",
    "            full_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(full_path, os.path.join(vox2_path, \"m4a\"))\n",
    "            parts = full_path.split(os.sep)\n",
    "            identity = parts[parts.index('aac') + 1]\n",
    "            data.append({\"file_name\": file, \"abs_filepath\": full_path, \"rel_filepath\": rel_path, \"identity\": identity})\n",
    "\n",
    "df_vox2 = pd.DataFrame(data)\n",
    "df_vox2 = df_vox2.sort_values(by=\"identity\", ascending=True, ignore_index=True)\n",
    "print(f\"vox2 shape: {df_vox2.shape}\")\n",
    "df_vox2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/12/7zfx6db53p5bf6btn6s9bqy40000gn/T/ipykernel_24951/1820632893.py:7: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  waveform, sr = librosa.load(abs_filepath, sr=8000)\n",
      "/Users/akanshagautam/Documents/MTech/ml_env/lib/python3.9/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load file: /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id02317/d4dinqezkKo/00383.m4a\n",
      "Failed to load file: /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id03030/haoNit7a4W0/00202.m4a\n",
      "Failed to load file: /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id03862/CJ8_3dVx7M4/00134.m4a\n",
      "Failed to load file: /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id04094/DRq5F2261Ko/00076.m4a\n",
      "Failed to load file: /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id04570/uLdhmrd9ULA/00369.m4a\n"
     ]
    }
   ],
   "source": [
    "# Load the waveform of voxCeleb2 audio files\n",
    "waveforms = []\n",
    "sampling_rates = []\n",
    "for index, row in df_vox2.iterrows():\n",
    "    abs_filepath = row['abs_filepath']\n",
    "    try:\n",
    "        waveform, sr = librosa.load(abs_filepath, sr=8000)\n",
    "        waveforms.append(waveform)\n",
    "        sampling_rates.append(sr)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load file: {abs_filepath}\")\n",
    "        waveforms.append([])\n",
    "        sampling_rates.append(0)\n",
    "\n",
    "df_vox2['waveform'] = waveforms\n",
    "df_vox2['sampling_rate'] = sampling_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train speakers: 50\n",
      "total test speakers: 50\n",
      "train dataframe shape: (14979, 6)\n",
      "test datafram shape: (14852, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>abs_filepath</th>\n",
       "      <th>rel_filepath</th>\n",
       "      <th>identity</th>\n",
       "      <th>waveform</th>\n",
       "      <th>sampling_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00045.m4a</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id03677/KVUf0J2LAaA/00045.m4a</td>\n",
       "      <td>../id03677/KVUf0J2LAaA/00045.m4a</td>\n",
       "      <td>id03677</td>\n",
       "      <td>[0.0083861165, 0.026233025, 0.034046598, 0.0303588, 0.013856893, 0.0012783967, 0.0017250986, 0.0048730876, -0.00033516577, -0.0056552067, -0.011302601, -0.01298482, -0.0021779481, 0.014832011, 0.026048947, 0.025501858, 0.020432718, 0.013180908, 0.002013075, -0.006209678, -0.007816484, -0.011034457, -0.017207533, -0.022063226, -0.022848576, -0.018088011, -0.007196877, 0.002857807, 0.0072496776, 0.0074041425, 0.009527016, 0.008918695, 0.0009219358, -0.0058669318, -0.0086981105, -0.0065184627, -0.0019151838, 0.0011819352, -0.0006137062, -0.0065999916, -0.008176668, -0.005478881, -0.006217077, -0.010039831, -0.0111991465, -0.009901179, -0.0076046647, -0.001035057, 0.0037649292, 0.0011443426, -0.0016406849, -0.0017831409, -0.0023553823, -0.0027396418, -0.00406816, -0.006611436, -0.007722936, -0.006516991, -0.0028938856, -0.0019445778, -0.0053353664, -0.009705443, -0.012247881, -0.008391787, -0.0030628918, -0.0026689013, -0.0044714846, -0.009046872, -0.014686999, -0.014612729, -0.010787064, -0.0069451327, -0.0028930919, -0.00074100774, -0.0014960687, -0.004205595, -0.0043240204, -0.0007527594, 0.0003778825, -0.0005301821, -0.0018669948, -0.0031750808, -0.0037565767, -0.0059486055, -0.00799782, -0.010575253, -0.012284346, -0.012557803, -0.01300272, -0.013386092, -0.008027082, 0.016552689, 0.06363653, 0.10263738, 0.09197521, 0.054114133, 0.01953137, -0.010047805, -0.018055957, -0.0061490694, ...]</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00051.m4a</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id03677/KVUf0J2LAaA/00051.m4a</td>\n",
       "      <td>../id03677/KVUf0J2LAaA/00051.m4a</td>\n",
       "      <td>id03677</td>\n",
       "      <td>[0.019320436, 0.03655121, 0.029749721, 0.0116706, -0.017885588, -0.024872515, -0.016596453, -0.0136062, -0.012669281, -0.011029578, -0.010837223, -0.0020937575, 0.013294073, 0.018867452, 0.014649382, 0.009456221, 0.0040021315, 0.0024738002, 0.0030363374, -0.0028475039, -0.014277519, -0.020596761, -0.020448424, -0.014353655, -0.003434096, -0.00040759938, -0.0043857237, -0.007430923, -0.007788301, -0.0042911624, -0.0007943013, 0.0008188444, -0.008010548, -0.01835487, -0.017136555, -0.015836425, -0.017722221, -0.023662876, -0.025325205, -0.022349458, -0.01444138, -0.0067230733, -0.0035214273, -7.1565155e-05, 9.09213e-05, 0.0014040149, 0.0023844764, 0.0020964593, -0.0010533715, -0.0079608355, -0.014743929, -0.015604187, -0.012256578, -0.010190288, -0.012769461, -0.011529008, -0.009154839, -0.00827192, -0.010606424, -0.008814596, -0.009751826, -0.01079187, -0.007000606, -0.0061468165, -0.012010712, -0.016527511, -0.016534181, -0.027671956, -0.02806696, -0.036965776, -0.035271376, -0.022242937, 0.052599117, 0.1211664, 0.097171135, 0.073913544, 0.040370885, 0.012573782, 0.0143562155, 0.022682253, 0.002599637, -0.02502123, -0.021831624, -0.0073738196, 0.017710513, 0.027382646, 0.007982532, -0.0053475723, 0.00792577, 0.029529434, 0.034833577, 0.025220616, -0.008876369, -0.03850408, -0.035472564, -0.018637605, -0.0059719305, 0.0009182376, 0.0021455786, 0.008929301, ...]</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name  \\\n",
       "0  00045.m4a   \n",
       "1  00051.m4a   \n",
       "\n",
       "                                                                                                       abs_filepath  \\\n",
       "0  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id03677/KVUf0J2LAaA/00045.m4a   \n",
       "1  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id03677/KVUf0J2LAaA/00051.m4a   \n",
       "\n",
       "                       rel_filepath identity  \\\n",
       "0  ../id03677/KVUf0J2LAaA/00045.m4a  id03677   \n",
       "1  ../id03677/KVUf0J2LAaA/00051.m4a  id03677   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              waveform  \\\n",
       "0  [0.0083861165, 0.026233025, 0.034046598, 0.0303588, 0.013856893, 0.0012783967, 0.0017250986, 0.0048730876, -0.00033516577, -0.0056552067, -0.011302601, -0.01298482, -0.0021779481, 0.014832011, 0.026048947, 0.025501858, 0.020432718, 0.013180908, 0.002013075, -0.006209678, -0.007816484, -0.011034457, -0.017207533, -0.022063226, -0.022848576, -0.018088011, -0.007196877, 0.002857807, 0.0072496776, 0.0074041425, 0.009527016, 0.008918695, 0.0009219358, -0.0058669318, -0.0086981105, -0.0065184627, -0.0019151838, 0.0011819352, -0.0006137062, -0.0065999916, -0.008176668, -0.005478881, -0.006217077, -0.010039831, -0.0111991465, -0.009901179, -0.0076046647, -0.001035057, 0.0037649292, 0.0011443426, -0.0016406849, -0.0017831409, -0.0023553823, -0.0027396418, -0.00406816, -0.006611436, -0.007722936, -0.006516991, -0.0028938856, -0.0019445778, -0.0053353664, -0.009705443, -0.012247881, -0.008391787, -0.0030628918, -0.0026689013, -0.0044714846, -0.009046872, -0.014686999, -0.014612729, -0.010787064, -0.0069451327, -0.0028930919, -0.00074100774, -0.0014960687, -0.004205595, -0.0043240204, -0.0007527594, 0.0003778825, -0.0005301821, -0.0018669948, -0.0031750808, -0.0037565767, -0.0059486055, -0.00799782, -0.010575253, -0.012284346, -0.012557803, -0.01300272, -0.013386092, -0.008027082, 0.016552689, 0.06363653, 0.10263738, 0.09197521, 0.054114133, 0.01953137, -0.010047805, -0.018055957, -0.0061490694, ...]   \n",
       "1                               [0.019320436, 0.03655121, 0.029749721, 0.0116706, -0.017885588, -0.024872515, -0.016596453, -0.0136062, -0.012669281, -0.011029578, -0.010837223, -0.0020937575, 0.013294073, 0.018867452, 0.014649382, 0.009456221, 0.0040021315, 0.0024738002, 0.0030363374, -0.0028475039, -0.014277519, -0.020596761, -0.020448424, -0.014353655, -0.003434096, -0.00040759938, -0.0043857237, -0.007430923, -0.007788301, -0.0042911624, -0.0007943013, 0.0008188444, -0.008010548, -0.01835487, -0.017136555, -0.015836425, -0.017722221, -0.023662876, -0.025325205, -0.022349458, -0.01444138, -0.0067230733, -0.0035214273, -7.1565155e-05, 9.09213e-05, 0.0014040149, 0.0023844764, 0.0020964593, -0.0010533715, -0.0079608355, -0.014743929, -0.015604187, -0.012256578, -0.010190288, -0.012769461, -0.011529008, -0.009154839, -0.00827192, -0.010606424, -0.008814596, -0.009751826, -0.01079187, -0.007000606, -0.0061468165, -0.012010712, -0.016527511, -0.016534181, -0.027671956, -0.02806696, -0.036965776, -0.035271376, -0.022242937, 0.052599117, 0.1211664, 0.097171135, 0.073913544, 0.040370885, 0.012573782, 0.0143562155, 0.022682253, 0.002599637, -0.02502123, -0.021831624, -0.0073738196, 0.017710513, 0.027382646, 0.007982532, -0.0053475723, 0.00792577, 0.029529434, 0.034833577, 0.025220616, -0.008876369, -0.03850408, -0.035472564, -0.018637605, -0.0059719305, 0.0009182376, 0.0021455786, 0.008929301, ...]   \n",
       "\n",
       "   sampling_rate  \n",
       "0           8000  \n",
       "1           8000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split voxceleb2 dataset into train and test\n",
    "all_speakers = sorted(df_vox2['identity'].unique())\n",
    "\n",
    "train_speakers = all_speakers[:50]\n",
    "test_speakers = all_speakers[50:100]\n",
    "print(\"total train speakers:\", len(train_speakers))\n",
    "print(\"total test speakers:\", len(test_speakers))\n",
    "\n",
    "spk2label = {spk: i for i, spk in enumerate(train_speakers)}\n",
    "\n",
    "df_train_vox2 = df_vox2[df_vox2['identity'].isin(train_speakers)].reset_index(drop=True)\n",
    "df_test_vox2 = df_vox2[df_vox2['identity'].isin(test_speakers)].reset_index(drop=True)\n",
    "print(\"train dataframe shape:\", df_train_vox2.shape)\n",
    "print(\"test datafram shape:\", df_test_vox2.shape)\n",
    "\n",
    "df_test_vox2.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a multi-speaker scenario dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = 1000\n",
    "multi_speaker_data = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sam1 = random.sample(df_train_vox2['identity'].unique().tolist(), 1)\n",
    "    sam2 = random.sample(df_test_vox2['identity'].unique().tolist(), 1)\n",
    "\n",
    "    x = df_train_vox2[df_train_vox2['identity'] == sam1[0]].sample(1).iloc[0]\n",
    "    y = df_test_vox2[df_test_vox2['identity'] == sam2[0]].sample(1).iloc[0]\n",
    "\n",
    "    wav1 = x['waveform']\n",
    "    wav2 = y['waveform']\n",
    "\n",
    "    min_len = min(len(wav1), len(wav2))\n",
    "    mixed = wav1[:min_len] + wav2[:min_len]\n",
    "    mixed_waveform =  mixed / np.max(np.abs(mixed))\n",
    "    multi_speaker_data.append({\n",
    "        \"waveform\": mixed_waveform,\n",
    "        \"x_waveform\": wav1[:min_len],\n",
    "        \"y_waveform\": wav2[:min_len],\n",
    "        \"x_identity\": x['identity'],\n",
    "        \"y_identity\": y['identity'],\n",
    "        \"x_abs_filepath\": x['abs_filepath'],\n",
    "        \"y_abs_filepath\": y['abs_filepath']\n",
    "    })\n",
    "\n",
    "df_mixed = pd.DataFrame(multi_speaker_data)\n",
    "df_mixed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A: Use the pre-trained SepFormer model to perform speaker separation and speech enhancement of each speaker on the created test set by analyzing these metrics: Signal to Interference Ratio (SIR), Signal to Artefacts Ratio (SAR), Signal to Distortion Ratio (SDR) and Perceptual Evaluation of Speech Quality (PESQ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Speaker Separation task on SepFormer model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch masknet.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch encoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch decoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: masknet, encoder, decoder\n"
     ]
    }
   ],
   "source": [
    "# Load SepFormer model\n",
    "model = separator.from_hparams(\n",
    "    source=\"speechbrain/sepformer-whamr\", \n",
    "    savedir='/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/pretrained_models/sepformer-whamr'\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/12/7zfx6db53p5bf6btn6s9bqy40000gn/T/ipykernel_24951/2317185987.py:25: FutureWarning: mir_eval.separation.bss_eval_sources\n",
      "\tDeprecated as of mir_eval version 0.8.\n",
      "\tIt will be removed in mir_eval version 0.9.\n",
      "  sdr, sir, sar, _ = bss_eval_sources(np.vstack(true_sources), np.vstack(estimated_sources))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The shape of estimated sources and the true sources should match.  reference_sources.shape = (2, 39048), estimated_sources.shape = (39048, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m true_source_y \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_waveform\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     23\u001b[0m true_sources \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([true_source_x, true_source_y])\n\u001b[0;32m---> 25\u001b[0m sdr, sir, sar, _ \u001b[38;5;241m=\u001b[39m \u001b[43mbss_eval_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_sources\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimated_sources\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m pesq_scores \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     28\u001b[0m     pesq(\u001b[38;5;241m16000\u001b[39m, true_sources[i], estimated_sources[i], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(true_sources))\n\u001b[1;32m     30\u001b[0m ]\n\u001b[1;32m     32\u001b[0m metrics_data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSDR\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(sdr), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSIR\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(sir), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAR\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(sar), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPESQ\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(pesq_scores)})\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/mir_eval/util.py:960\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[1;32m    954\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of mir_eval version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in mir_eval version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion_removed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    957\u001b[0m     category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    958\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[1;32m    959\u001b[0m )\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/mir_eval/separation.py:212\u001b[0m, in \u001b[0;36mbss_eval_sources\u001b[0;34m(reference_sources, estimated_sources, compute_permutation)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reference_sources\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    210\u001b[0m     reference_sources \u001b[38;5;241m=\u001b[39m reference_sources[np\u001b[38;5;241m.\u001b[39mnewaxis, :]\n\u001b[0;32m--> 212\u001b[0m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_sources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimated_sources\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# If empty matrices were supplied, return empty lists (special case)\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reference_sources\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m estimated_sources\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/mir_eval/separation.py:80\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(reference_sources, estimated_sources)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that the input data to a metric are valid, and throws helpful\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03merrors if not.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reference_sources\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m estimated_sources\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of estimated sources and the true \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msources should match.  reference_sources.shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m= \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, estimated_sources.shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m= \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(reference_sources\u001b[38;5;241m.\u001b[39mshape, estimated_sources\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reference_sources\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m estimated_sources\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of dimensions is too high (must be less \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan 3). reference_sources.ndim = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimated_sources.ndim \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m= \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(reference_sources\u001b[38;5;241m.\u001b[39mndim, estimated_sources\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m     93\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The shape of estimated sources and the true sources should match.  reference_sources.shape = (2, 39048), estimated_sources.shape = (39048, 2)"
     ]
    }
   ],
   "source": [
    "def extract_embedding(waveform):\n",
    "    waveform = torch.tensor(waveform)\n",
    "    if waveform.ndim > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    sources_separation = model.separate_batch(waveform.unsqueeze(0))\n",
    "    return sources_separation.squeeze(0)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "similarities = []\n",
    "metrics_data = []\n",
    "\n",
    "for index, row in df_mixed.iterrows():\n",
    "    mixed_waveform = torch.tensor(row['waveform']).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        estimated_sources = model.separate_batch(mixed_waveform)\n",
    "\n",
    "    estimated_sources = estimated_sources.squeeze().cpu().numpy()\n",
    "    true_source_x = row['x_waveform']\n",
    "    true_source_y = row['y_waveform']\n",
    "\n",
    "    true_sources = np.array([true_source_x, true_source_y])\n",
    "\n",
    "    sdr, sir, sar, _ = bss_eval_sources(np.vstack(true_sources), np.vstack(estimated_sources))\n",
    "\n",
    "    pesq_scores = [\n",
    "        pesq(16000, true_sources[i], estimated_sources[i], 'wb')\n",
    "        for i in range(len(true_sources))\n",
    "    ]\n",
    "\n",
    "    metrics_data.append({'SDR': np.mean(sdr), 'SIR': np.mean(sir), 'SAR': np.mean(sar), 'PESQ': np.mean(pesq_scores)})\n",
    "\n",
    "metrics_data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Speaker Enhancement task on SepFormer model on the test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
