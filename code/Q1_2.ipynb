{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Understanding\n",
    "## Assignment 2\n",
    "#### Question 1.2\n",
    "### Akansha Gautam (M23CSA506)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import subprocess\n",
    "from transformers import Wav2Vec2Processor, HubertForCTC\n",
    "from sklearn.metrics import roc_curve\n",
    "from transformers import AutoModel, AutoProcessor, AutoFeatureExtractor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile as wav\n",
    "import torch.nn.functional as F\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2Processor, WavLMModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import UniSpeechSatModel, UniSpeechSatConfig, Wav2Vec2FeatureExtractor\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import brentq\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pytube import YouTube\n",
    "import ffmpeg\n",
    "import os\n",
    "import loralib as lora\n",
    "from loralib import Linear as LoRALinear\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load vox1 audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vox1 shape: (4874, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>abs_filepath</th>\n",
       "      <th>rel_filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00005.wav</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10295/nt7dNRvlEHE/00005.wav</td>\n",
       "      <td>id10295/nt7dNRvlEHE/00005.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00004.wav</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10295/nt7dNRvlEHE/00004.wav</td>\n",
       "      <td>id10295/nt7dNRvlEHE/00004.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001.wav</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10295/nt7dNRvlEHE/00001.wav</td>\n",
       "      <td>id10295/nt7dNRvlEHE/00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00003.wav</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10295/nt7dNRvlEHE/00003.wav</td>\n",
       "      <td>id10295/nt7dNRvlEHE/00003.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00002.wav</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10295/nt7dNRvlEHE/00002.wav</td>\n",
       "      <td>id10295/nt7dNRvlEHE/00002.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name  \\\n",
       "0  00005.wav   \n",
       "1  00004.wav   \n",
       "2  00001.wav   \n",
       "3  00003.wav   \n",
       "4  00002.wav   \n",
       "\n",
       "                                                                                                                               abs_filepath  \\\n",
       "0  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10295/nt7dNRvlEHE/00005.wav   \n",
       "1  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10295/nt7dNRvlEHE/00004.wav   \n",
       "2  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10295/nt7dNRvlEHE/00001.wav   \n",
       "3  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10295/nt7dNRvlEHE/00003.wav   \n",
       "4  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10295/nt7dNRvlEHE/00002.wav   \n",
       "\n",
       "                    rel_filepath  \n",
       "0  id10295/nt7dNRvlEHE/00005.wav  \n",
       "1  id10295/nt7dNRvlEHE/00004.wav  \n",
       "2  id10295/nt7dNRvlEHE/00001.wav  \n",
       "3  id10295/nt7dNRvlEHE/00003.wav  \n",
       "4  id10295/nt7dNRvlEHE/00002.wav  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vox1_path = \"/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for root, dirs, files in os.walk(vox1_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            # path = os.path.join(dirs)\n",
    "            full_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(full_path, os.path.join(vox1_path, \"wav\"))\n",
    "            data.append({\"file_name\": file, \"abs_filepath\": full_path, \"rel_filepath\": rel_path})\n",
    "\n",
    "df_vox1 = pd.DataFrame(data)\n",
    "print(f\"vox1 shape: {df_vox1.shape}\")\n",
    "df_vox1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the list of trial pairs of vox celeb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vox1 shape: (37611, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>path1</th>\n",
       "      <th>path2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>id10270/x6uYqmx31kE/00001.wav</td>\n",
       "      <td>id10270/8jEAjG6SegY/00008.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>id10270/x6uYqmx31kE/00001.wav</td>\n",
       "      <td>id10300/ize_eiCFEg0/00003.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>id10270/x6uYqmx31kE/00001.wav</td>\n",
       "      <td>id10270/GWXujl-xAVM/00017.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>id10270/x6uYqmx31kE/00001.wav</td>\n",
       "      <td>id10273/0OCW1HUxZyg/00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>id10270/x6uYqmx31kE/00001.wav</td>\n",
       "      <td>id10270/8jEAjG6SegY/00022.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                          path1                          path2\n",
       "0      1  id10270/x6uYqmx31kE/00001.wav  id10270/8jEAjG6SegY/00008.wav\n",
       "1      0  id10270/x6uYqmx31kE/00001.wav  id10300/ize_eiCFEg0/00003.wav\n",
       "2      1  id10270/x6uYqmx31kE/00001.wav  id10270/GWXujl-xAVM/00017.wav\n",
       "3      0  id10270/x6uYqmx31kE/00001.wav  id10273/0OCW1HUxZyg/00001.wav\n",
       "4      1  id10270/x6uYqmx31kE/00001.wav  id10270/8jEAjG6SegY/00022.wav"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 if the two audio clips are from the same speaker, 0 otherwise\n",
    "\n",
    "vox_celeb1_trial_pairs = '/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/list_of_trial_pairs_vox_celeb1.txt'\n",
    "\n",
    "df_vox_celeb1_trial_pairs = pd.read_csv(vox_celeb1_trial_pairs, sep=' ', header=None)\n",
    "df_vox_celeb1_trial_pairs.columns = ['label', 'path1', 'path2']\n",
    "\n",
    "print(f\"vox1 shape: {df_vox_celeb1_trial_pairs.shape}\")\n",
    "df_vox_celeb1_trial_pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load audio files with trial pairs of vox celeb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged shape: (37611, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>path1</th>\n",
       "      <th>path1_file_name</th>\n",
       "      <th>path1_abs_filepath</th>\n",
       "      <th>path2</th>\n",
       "      <th>path2_file_name</th>\n",
       "      <th>path2_abs_filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>id10270/x6uYqmx31kE/00001.wav</td>\n",
       "      <td>00001.wav</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10270/x6uYqmx31kE/00001.wav</td>\n",
       "      <td>id10270/8jEAjG6SegY/00008.wav</td>\n",
       "      <td>00008.wav</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10270/8jEAjG6SegY/00008.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>id10270/x6uYqmx31kE/00001.wav</td>\n",
       "      <td>00001.wav</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10270/x6uYqmx31kE/00001.wav</td>\n",
       "      <td>id10300/ize_eiCFEg0/00003.wav</td>\n",
       "      <td>00003.wav</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10300/ize_eiCFEg0/00003.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                          path1 path1_file_name  \\\n",
       "0      1  id10270/x6uYqmx31kE/00001.wav       00001.wav   \n",
       "1      0  id10270/x6uYqmx31kE/00001.wav       00001.wav   \n",
       "\n",
       "                                                                                                                         path1_abs_filepath  \\\n",
       "0  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10270/x6uYqmx31kE/00001.wav   \n",
       "1  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10270/x6uYqmx31kE/00001.wav   \n",
       "\n",
       "                           path2 path2_file_name  \\\n",
       "0  id10270/8jEAjG6SegY/00008.wav       00008.wav   \n",
       "1  id10300/ize_eiCFEg0/00003.wav       00003.wav   \n",
       "\n",
       "                                                                                                                         path2_abs_filepath  \n",
       "0  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10270/8jEAjG6SegY/00008.wav  \n",
       "1  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/Speech2025Datasets/vox1/wav/id10300/ize_eiCFEg0/00003.wav  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pairs_with_path1 = df_vox_celeb1_trial_pairs.merge(\n",
    "    df_vox1[['rel_filepath', 'file_name', 'abs_filepath']],\n",
    "    left_on='path1',\n",
    "    right_on='rel_filepath',\n",
    "    how='left'\n",
    ").rename(columns={'file_name': 'path1_file_name', 'abs_filepath': 'path1_abs_filepath'}).drop(columns=['rel_filepath'])\n",
    "\n",
    "df_final_pairs = df_pairs_with_path1.merge(\n",
    "    df_vox1[['rel_filepath', 'file_name', 'abs_filepath']],\n",
    "    left_on='path2',\n",
    "    right_on='rel_filepath',\n",
    "    how='left'\n",
    ").rename(columns={'file_name': 'path2_file_name', 'abs_filepath': 'path2_abs_filepath'}).drop(columns=['rel_filepath'])\n",
    "\n",
    "df_final_pairs = df_final_pairs[\n",
    "    ['label', 'path1', 'path1_file_name', 'path1_abs_filepath', 'path2', 'path2_file_name', 'path2_abs_filepath']\n",
    "]\n",
    "\n",
    "print(f\"Final merged shape: {df_final_pairs.shape}\")\n",
    "df_final_pairs.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained processor and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Speaker Verification (Evaluation) using the list of trial pairs - VoxCeleb1 (cleaned) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 7)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduced the size of trial pairs to 500\n",
    "df_final = df_final_pairs.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_final = df_final.head(500)\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [08:41<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "def extract_embedding(audio_path):\n",
    "\n",
    "    waveform, _ = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    if waveform.ndim > 1:\n",
    "        waveform = waveform[0]\n",
    "\n",
    "    inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_values = inputs[\"input_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "similarities = []\n",
    "\n",
    "for _, row in tqdm(df_final.iterrows(), total=len(df_final)):\n",
    "    emb1 = extract_embedding(row['path1_abs_filepath'])\n",
    "    emb2 = extract_embedding(row['path2_abs_filepath'])\n",
    "\n",
    "    similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
    "    similarities.append(similarity)\n",
    "    predicted_label = 1 if similarity > 0.7 else 0\n",
    "\n",
    "    y_true.append(row['label'])\n",
    "    y_pred.append(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker verification accuracy using UniSpeech-SAT model: 0.488\n",
      "Equal Error Rate in % using UniSpeech-SAT model: 0.488\n",
      "TAR@1% FAR using UniSpeech-SAT model: 0.488\n"
     ]
    }
   ],
   "source": [
    "# Calculate Speaker Verification Accuracy\n",
    "accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "print(f\"Speaker verification accuracy using UniSpeech-SAT model: {accuracy}\")\n",
    "\n",
    "# Calculate Equal Error Rate (ERR) in %\n",
    "false_positive_rate, true_positive_rate, _ = roc_curve(y_true, similarities)\n",
    "false_negative_rate = 1 - true_positive_rate\n",
    "eer_function = interp1d(false_positive_rate, false_negative_rate)\n",
    "eer = (brentq(lambda x: eer_function(x) - x, 0.0, 1.0) * 100)\n",
    "print(f\"Equal Error Rate in % using UniSpeech-SAT model: {accuracy}\")\n",
    "\n",
    "# Calcualte TAR@1% FAR\n",
    "target_far = 0.01\n",
    "tar_at_1_far = 0.0\n",
    "for i in range(len(false_positive_rate)):\n",
    "    if false_positive_rate[i] <= target_far:\n",
    "        tar_at_1_far = true_positive_rate[i] \n",
    "\n",
    "tar_at_1_far *= 100 \n",
    "print(f\"TAR@1% FAR using UniSpeech-SAT model: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the speaker verification model using LoRA and ArcFace loss with voxceleb2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vox2 shape: (36237, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>abs_filepath</th>\n",
       "      <th>rel_filepath</th>\n",
       "      <th>identity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00043.m4a</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id00017/OLguY5ofUrY/00043.m4a</td>\n",
       "      <td>../id00017/OLguY5ofUrY/00043.m4a</td>\n",
       "      <td>id00017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00179.m4a</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id00017/lvX8diWgriA/00179.m4a</td>\n",
       "      <td>../id00017/lvX8diWgriA/00179.m4a</td>\n",
       "      <td>id00017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name  \\\n",
       "0  00043.m4a   \n",
       "1  00179.m4a   \n",
       "\n",
       "                                                                                                       abs_filepath  \\\n",
       "0  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id00017/OLguY5ofUrY/00043.m4a   \n",
       "1  /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id00017/lvX8diWgriA/00179.m4a   \n",
       "\n",
       "                       rel_filepath identity  \n",
       "0  ../id00017/OLguY5ofUrY/00043.m4a  id00017  \n",
       "1  ../id00017/lvX8diWgriA/00179.m4a  id00017  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the vox celeb2 audio files path\n",
    "vox2_path = '/Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac'\n",
    "data = []\n",
    "\n",
    "for root, dirs, files in os.walk(vox2_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".m4a\"):\n",
    "            full_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(full_path, os.path.join(vox2_path, \"m4a\"))\n",
    "            parts = full_path.split(os.sep)\n",
    "            identity = parts[parts.index('aac') + 1]\n",
    "            data.append({\"file_name\": file, \"abs_filepath\": full_path, \"rel_filepath\": rel_path, \"identity\": identity})\n",
    "\n",
    "df_vox2 = pd.DataFrame(data)\n",
    "df_vox2 = df_vox2.sort_values(by=\"identity\", ascending=True, ignore_index=True)\n",
    "print(f\"vox2 shape: {df_vox2.shape}\")\n",
    "df_vox2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/12/7zfx6db53p5bf6btn6s9bqy40000gn/T/ipykernel_9574/2654778886.py:7: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  waveform, sr = librosa.load(abs_filepath, sr=16000)\n",
      "/Users/akanshagautam/Documents/MTech/ml_env/lib/python3.9/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load file: /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id02317/d4dinqezkKo/00383.m4a\n",
      "Failed to load file: /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id03030/haoNit7a4W0/00202.m4a\n",
      "Failed to load file: /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id03862/CJ8_3dVx7M4/00134.m4a\n",
      "Failed to load file: /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id04094/DRq5F2261Ko/00076.m4a\n",
      "Failed to load file: /Users/akanshagautam/Documents/MTech/Speech Understanding/Assignment 2/dataset/aac/id04570/uLdhmrd9ULA/00369.m4a\n"
     ]
    }
   ],
   "source": [
    "# Load the waveform of voxCeleb2 audio files\n",
    "waveforms = []\n",
    "sampling_rates = []\n",
    "for index, row in df_vox2.iterrows():\n",
    "    abs_filepath = row['abs_filepath']\n",
    "    try:\n",
    "        waveform, sr = librosa.load(abs_filepath, sr=16000)\n",
    "        waveforms.append(waveform)\n",
    "        sampling_rates.append(sr)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load file: {abs_filepath}\")\n",
    "        waveforms.append([])\n",
    "        sampling_rates.append(0)\n",
    "\n",
    "df_vox2['waveform'] = waveforms\n",
    "df_vox2['sampling_rate'] = sampling_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train speakers: 100\n",
      "total test speakers: 18\n",
      "train dataframe shape: (29831, 6)\n",
      "test datafram shape: (6406, 6)\n"
     ]
    }
   ],
   "source": [
    "# Split voxceleb2 dataset into train and test\n",
    "all_speakers = sorted(df_vox2['identity'].unique())\n",
    "\n",
    "train_speakers = all_speakers[:100]\n",
    "test_speakers = all_speakers[100:]\n",
    "print(\"total train speakers:\", len(train_speakers))\n",
    "print(\"total test speakers:\", len(test_speakers))\n",
    "\n",
    "spk2label = {spk: i for i, spk in enumerate(train_speakers)}\n",
    "\n",
    "df_train_vox2 = df_vox2[df_vox2['identity'].isin(train_speakers)].reset_index(drop=True)\n",
    "df_test_vox2 = df_vox2[df_vox2['identity'].isin(test_speakers)].reset_index(drop=True)\n",
    "print(\"train dataframe shape:\", df_train_vox2.shape)\n",
    "print(\"test datafram shape:\", df_test_vox2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "model = model.to(device)\n",
    "\n",
    "for layer in model.encoder.layers:\n",
    "    layer.attention.q_proj = LoRALinear(layer.attention.q_proj.in_features, layer.attention.q_proj.out_features)\n",
    "    layer.attention.v_proj = LoRALinear(layer.attention.v_proj.in_features, layer.attention.v_proj.out_features)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_arcface_weights(embedding_dim, num_classes):\n",
    "    W = torch.nn.Parameter(torch.randn(embedding_dim, num_classes))\n",
    "    torch.nn.init.xavier_uniform_(W)\n",
    "    return W\n",
    "\n",
    "def arcface_loss_fn(embeddings, labels, W, scale=30.0, margin=0.5):\n",
    "    embeddings = F.normalize(embeddings)\n",
    "    W = F.normalize(W, dim=0)\n",
    "    cosine = torch.matmul(embeddings, W)\n",
    "    theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "    target_logits = torch.cos(theta + margin)\n",
    "    one_hot = F.one_hot(labels, num_classes=W.shape[1]).float().to(embeddings.device)\n",
    "    logits = cosine * (1 - one_hot) + target_logits * one_hot\n",
    "    logits *= scale\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "arcface_W = init_arcface_weights(model.config.hidden_size, len(train_speakers)).to(device)\n",
    "optimizer = torch.optim.AdamW(list(model.parameters()) + [arcface_W], lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epoch 1 ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [11:13<00:00,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 8.1646\n",
      "---- Epoch 2 ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [11:06<00:00,  5.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 8.0290\n",
      "---- Epoch 3 ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [10:33<00:00,  5.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 15.6219\n",
      "---- Epoch 4 ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [10:49<00:00,  5.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 16.9534\n",
      "---- Epoch 5 ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [10:53<00:00,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 14.0448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "max_duration_sec = 3\n",
    "sample_rate = 16000\n",
    "max_len = sample_rate * max_duration_sec\n",
    "batch_size = 8\n",
    "\n",
    "df_train_vox2_updated = df_train_vox2.head(1000)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"---- Epoch {epoch+1} ----\")\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for i in tqdm(range(0, len(df_train_vox2_updated), batch_size)):\n",
    "        batch_rows = df_train_vox2_updated.iloc[i:i+batch_size]\n",
    "        input_tensors = []\n",
    "        label_tensors = []\n",
    "\n",
    "        for _, row in batch_rows.iterrows():\n",
    "            waveform = row['waveform']\n",
    "            identity = row['identity']\n",
    "\n",
    "            if len(waveform) == 0 or identity not in spk2label:\n",
    "                continue\n",
    "\n",
    "            if len(waveform) > max_len:\n",
    "                waveform = waveform[:max_len]\n",
    "            else:\n",
    "                waveform = np.pad(waveform, (0, max_len - len(waveform)))\n",
    "\n",
    "            inputs = feature_extractor(torch.tensor(waveform), sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "            input_tensors.append(inputs[\"input_values\"].squeeze(0))\n",
    "            label_tensors.append(spk2label[identity])\n",
    "\n",
    "        if not input_tensors:\n",
    "            continue\n",
    "\n",
    "        inputs_batch = torch.stack(input_tensors).to(device)\n",
    "        labels_batch = torch.tensor(label_tensors).to(device)\n",
    "\n",
    "        outputs = model(input_values=inputs_batch)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        loss = arcface_loss_fn(embeddings, labels_batch, arcface_W)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / (len(df_train_vox2_updated) // batch_size)\n",
    "    print(f\"Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [15:19<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model on vox-celeb-1 dataset\n",
    "\n",
    "def extract_embedding(audio_path):\n",
    "\n",
    "    waveform, _ = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    if waveform.ndim > 1:\n",
    "        waveform = waveform[0]\n",
    "\n",
    "    inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_values = inputs[\"input_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "similarities = []\n",
    "\n",
    "for _, row in tqdm(df_final.iterrows(), total=len(df_final)):\n",
    "    emb1 = extract_embedding(row['path1_abs_filepath'])\n",
    "    emb2 = extract_embedding(row['path2_abs_filepath'])\n",
    "\n",
    "    similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
    "    similarities.append(similarity)\n",
    "    predicted_label = 1 if similarity > 0.7 else 0\n",
    "\n",
    "    y_true.append(row['label'])\n",
    "    y_pred.append(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker verification accuracy using UniSpeech-SAT model: 0.5\n",
      "Equal Error Rate in % using UniSpeech-SAT model: 0.5\n",
      "TAR@1% FAR using UniSpeech-SAT model: 0.5\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Calculate Speaker Verification Accuracy\n",
    "accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "print(f\"Speaker verification accuracy using UniSpeech-SAT model: {accuracy}\")\n",
    "\n",
    "# Calculate Equal Error Rate (ERR) in %\n",
    "false_positive_rate, true_positive_rate, _ = roc_curve(y_true, similarities)\n",
    "false_negative_rate = 1 - true_positive_rate\n",
    "eer_function = interp1d(false_positive_rate, false_negative_rate)\n",
    "eer = (brentq(lambda x: eer_function(x) - x, 0.0, 1.0) * 100)\n",
    "print(f\"Equal Error Rate in % using UniSpeech-SAT model: {accuracy}\")\n",
    "\n",
    "# Calcualte TAR@1% FAR\n",
    "target_far = 0.01\n",
    "tar_at_1_far = 0.0\n",
    "for i in range(len(false_positive_rate)):\n",
    "    if false_positive_rate[i] <= target_far:\n",
    "        tar_at_1_far = true_positive_rate[i] \n",
    "\n",
    "tar_at_1_far *= 100 \n",
    "print(f\"TAR@1% FAR using UniSpeech-SAT model: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
